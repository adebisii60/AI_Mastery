{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Same thing for Gemini\n",
        "import os\n",
        "from google import genai\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "client = genai.Client()\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\", contents=\"Explain Generative AI in 2 bullet points.\"\n",
        "    )\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "37255c22-659c-4c37-d91c-21d72cc8b9f0",
        "id": "yZ-cPvgc5Glx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'userdata' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-670486483.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GEMINI_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GEMINI_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'userdata' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb6rdwlCsCGt"
      },
      "source": [
        "# Using ChatGPT or Google Gemini with Python for real-world tasks using thier APIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2evPp14fy258"
      },
      "outputs": [],
      "source": [
        "!pip install openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8TqiDPJzzkTN",
        "outputId": "6c011451-1110-4b1e-a265-6ffe9b69bae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.16.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install openai==2.12.0 --quiet   # this how to install a very specific version of the package"
      ],
      "metadata": {
        "id": "XfTmRA9-z6R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N_Lly09l0P9"
      },
      "source": [
        "## Optional: Install Google Gemini\n",
        "\n",
        "Google Gemini API is free (till now). You can get a key [here](https://aistudio.google.com/app/apikey), just need to sign in with your google account. Gemini may not be available fully in EU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKbTnx8pl8kt"
      },
      "outputs": [],
      "source": [
        "# !pip install -q -U google-generativeai\n",
        "!pip install -q -U google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiwGjVWK4q6F"
      },
      "source": [
        "## Load OpenAI API Credentials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryheOZuXxa41"
      },
      "outputs": [],
      "source": [
        "# Step 2: import keys for openai and Assign environment variable\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Call the Responses API for accessing the model\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5-nano\", # gpt-5.2-2025-12-11\n",
        "    input=\"Explain Generative AI in 2 bullet points.\"\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsKN2CKh_Z7k",
        "outputId": "bd4c2a49-9a64-44af-f685-43010c5a3edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Generative AI refers to models trained on large datasets to create new content (text, images, music, code) that resembles the training data, not just classify or predict.\n",
            "- It uses architectures like transformers, diffusion models, or GANs and is guided by prompts or constraints to generate outputs, enabling creativity and automation but raising concerns about quality, bias, copyright, and misuse.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEM3KT1mUyjh",
        "outputId": "f71af80f-0090-4730-829a-5a3c4fbe36ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(id='resp_06da48f642dcfb5a006989f12f0c7c81a09c00732552587950', created_at=1770647855.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-nano-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_06da48f642dcfb5a006989f12f883481a0ac65b21e92c3876c', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_06da48f642dcfb5a006989f135a7bc81a0af78661b1a3c6bfd', content=[ResponseOutputText(annotations=[], text='- Generative AI models learn the patterns in existing data to produce new, plausible content (text, images, audio, code) that isn’t just copied but creatively generated.\\n- It relies on generative architectures (GANs, VAEs, autoregressive or diffusion models) trained on large datasets, enabling creative automation but bringing challenges like bias, misinformation, copyright, and the need for quality control.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, completed_at=1770647865.0, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=16, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=629, output_tokens_details=OutputTokensDetails(reasoning_tokens=512), total_tokens=645), user=None, billing={'payer': 'developer'}, frequency_penalty=0.0, presence_penalty=0.0, store=True)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS7koM2emZ_M"
      },
      "source": [
        "## Load Gemini API credentials\n",
        "\n",
        "Run this section only if you are using Google Gemini"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Same thing for Gemini\n",
        "import os\n",
        "from google import genai\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "client = genai.Client()\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\", contents=\"Explain Generative AI in 2 bullet points.\"\n",
        "    )\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "2NaVRkFIM0Mf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b91e3f-cafb-43b6-ae95-d22b273bca95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are two bullet points explaining Generative AI:\n",
            "\n",
            "*   **Creates New Content:** Generative AI models are designed to produce **new, original content** (like text, images, audio, or code) that is realistic and coherent, rather than just retrieving existing information.\n",
            "*   **Learns from Data:** They achieve this by learning complex **patterns and structures** from vast amounts of existing data during training, allowing them to then generate novel examples that conform to those learned patterns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "rR3QZzF8aX_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e0bf02-8805-4f62-b2bb-9c9360099d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerateContentResponse(\n",
              "  automatic_function_calling_history=[],\n",
              "  candidates=[\n",
              "    Candidate(\n",
              "      content=Content(\n",
              "        parts=[\n",
              "          Part(\n",
              "            text=\"\"\"Here are two bullet points explaining Generative AI:\n",
              "\n",
              "*   **Creates New Content:** Generative AI models are designed to produce **new, original content** (like text, images, audio, or code) that is realistic and coherent, rather than just retrieving existing information.\n",
              "*   **Learns from Data:** They achieve this by learning complex **patterns and structures** from vast amounts of existing data during training, allowing them to then generate novel examples that conform to those learned patterns.\"\"\"\n",
              "          ),\n",
              "        ],\n",
              "        role='model'\n",
              "      ),\n",
              "      finish_reason=<FinishReason.STOP: 'STOP'>,\n",
              "      index=0\n",
              "    ),\n",
              "  ],\n",
              "  model_version='gemini-2.5-flash',\n",
              "  response_id='1DeKaebpEbWz-8YPypTvmAI',\n",
              "  sdk_http_response=HttpResponse(\n",
              "    headers=<dict len=10>\n",
              "  ),\n",
              "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
              "    candidates_token_count=99,\n",
              "    prompt_token_count=11,\n",
              "    prompt_tokens_details=[\n",
              "      ModalityTokenCount(\n",
              "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
              "        token_count=11\n",
              "      ),\n",
              "    ],\n",
              "    thoughts_token_count=945,\n",
              "    total_token_count=1055\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aivThNd-gzf7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDWhgxCy5bA6"
      },
      "source": [
        "## Create ChatGPT and Google Gemini Chat Completion Access Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA9gVCwK0WKd"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=\"gpt\"):\n",
        "  if model == \"gpt\":\n",
        "    client = OpenAI()\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-5-nano\",\n",
        "        input=prompt\n",
        "    )\n",
        "    return response.output_text\n",
        "\n",
        "  if model == \"gemini\":\n",
        "    client = genai.Client()\n",
        "    response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\", contents=prompt\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "  else:\n",
        "    return \"LLM not configured! Please configure logic for specific model in get_completion()\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pW7KmVygzf_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6L1tETs06Go"
      },
      "source": [
        "## Exercise-1: Text Generation with both ChatGPT and Google Gemini\n",
        "- Get ChatGPT to generate text by asking it to write a story\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0VDZwEG1B8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ee4900-4c6f-409a-f54f-1e2ff7584f08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here’s a simple way to think about it:\n",
            "\n",
            "- Encoder: Reads the input and makes a compact summary or set of features. It “compresses” information into a hidden representation.\n",
            "\n",
            "- Decoder: Takes that summary and writes the output, one piece at a time. It uses the encoder’s summary to generate something new.\n",
            "\n",
            "Simple analogy:\n",
            "- Encoder is like a reader who writes a short note capturing the main idea.\n",
            "- Decoder is like a writer who uses that note to compose a new message (a translation, a summary, or an answer).\n",
            "\n",
            "Three common setups:\n",
            "- Encoder-only models (e.g., BERT): good at understanding tasks. They read text and produce representations used for classification or matching, not for generating new text.\n",
            "- Decoder-only models (e.g., GPT): good at generating text. They read nothing explicitly to begin with and then produce output token by token, based on what they’ve generated so far.\n",
            "- Encoder–decoder models (e.g., T5, the original seq2seq): combine both. The encoder reads the input (like a source sentence), and the decoder generates the output (like a translated sentence or a summary) using the encoder’s summary.\n",
            "\n",
            "When to use which:\n",
            "- Understanding tasks (classification, similarity): encoder-only.\n",
            "- Full generation (writing, completing text): decoder-only.\n",
            "- Translation, summarization, or any task that maps input to a specific output: encoder–decoder.\n"
          ]
        }
      ],
      "source": [
        "response = get_completion(\"Explain the difference between encoder and decoder models in simple way.\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "065uztCC8m1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c7b75f-0b2e-4a02-f1a9-57e7880d3b27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine you have a complex idea you want to explain to someone in a different language.\n",
            "\n",
            "**Encoder and Decoder models work like a two-step communication process:**\n",
            "\n",
            "---\n",
            "\n",
            "### The **Encoder** Model: **The \"Listener\" / \"Reader\"**\n",
            "\n",
            "*   **What it does:** It **reads and understands** the input information. Its job is to capture the essence, meaning, and important features of whatever you give it.\n",
            "*   **Input:** The original, raw data or sequence (e.g., a sentence in French, an image, a long paragraph).\n",
            "*   **Output:** A condensed, numerical representation (often called a \"context vector\" or \"latent representation\") that holds all the critical information from the input. Think of this as the *thought* or *gist* of the input, stripped of its original format but retaining its meaning.\n",
            "*   **Analogy:** You're listening to someone speak French. Your brain acts as the encoder, understanding the words, grammar, and overall meaning, and forming an internal \"thought\" about what was said, without yet translating it into English.\n",
            "\n",
            "---\n",
            "\n",
            "### The **Decoder** Model: **The \"Speaker\" / \"Writer\"**\n",
            "\n",
            "*   **What it does:** It **generates** new information based on the understanding it received from the encoder. It takes that \"thought\" and expands it into a new, meaningful output.\n",
            "*   **Input:** Primarily the condensed \"thought\" (context vector) from the encoder. It also often uses its *own previously generated outputs* to decide what to generate next, building the output sequence step by step.\n",
            "*   **Output:** A new sequence or piece of data that is a transformation of the original input (e.g., the sentence translated into English, a caption describing the image, a summarized paragraph).\n",
            "*   **Analogy:** Now that your brain (the encoder) has formed the internal \"thought\" from the French sentence, your brain (now acting as the decoder) takes that thought and constructs an English sentence to express it.\n",
            "\n",
            "---\n",
            "\n",
            "### **The Key Difference in Simple Terms:**\n",
            "\n",
            "*   **Encoder:** **Understands** the input and turns it into a compact, meaningful internal representation.\n",
            "    *   *Role:* Compression, Feature Extraction, Understanding.\n",
            "*   **Decoder:** **Generates** an output sequence based on that internal representation.\n",
            "    *   *Role:* Generation, Reconstruction, Translation.\n",
            "\n",
            "**Think of it like this:**\n",
            "\n",
            "1.  **Encoder:** You read a complicated instruction manual and distill its main points into a single mental note.\n",
            "2.  **Decoder:** You take that mental note and write a simple, easy-to-follow instruction for someone else.\n",
            "\n",
            "They are often used together in **\"sequence-to-sequence\" (Seq2Seq)** models for tasks like machine translation, text summarization, image captioning, and speech recognition. The encoder processes the input sequence, and the decoder generates the output sequence based on the encoder's understanding.\n"
          ]
        }
      ],
      "source": [
        "# with Gemini\n",
        "response = get_completion(\"Explain the difference between encoder and decoder models in simple way.\", model='gemini')\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfnG0qe3gzgC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEVszGl8gzgD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TFZjzuGjCOw"
      },
      "source": [
        "## Exercise-2: Let's try out Zero Shot Prompting!\n",
        "\n",
        "- Let's get the model to answer the question about Generative AI\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBm9ef4MgzgD"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response))  # beautified response from the Gemini Model"
      ],
      "metadata": {
        "id": "KKPMMzwaHXN8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "016f7cfb-4a84-4e03-8fde-db8bf1de3b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Imagine you have a complex idea you want to explain to someone in a different language.\n\n**Encoder and Decoder models work like a two-step communication process:**\n\n---\n\n### The **Encoder** Model: **The \"Listener\" / \"Reader\"**\n\n*   **What it does:** It **reads and understands** the input information. Its job is to capture the essence, meaning, and important features of whatever you give it.\n*   **Input:** The original, raw data or sequence (e.g., a sentence in French, an image, a long paragraph).\n*   **Output:** A condensed, numerical representation (often called a \"context vector\" or \"latent representation\") that holds all the critical information from the input. Think of this as the *thought* or *gist* of the input, stripped of its original format but retaining its meaning.\n*   **Analogy:** You're listening to someone speak French. Your brain acts as the encoder, understanding the words, grammar, and overall meaning, and forming an internal \"thought\" about what was said, without yet translating it into English.\n\n---\n\n### The **Decoder** Model: **The \"Speaker\" / \"Writer\"**\n\n*   **What it does:** It **generates** new information based on the understanding it received from the encoder. It takes that \"thought\" and expands it into a new, meaningful output.\n*   **Input:** Primarily the condensed \"thought\" (context vector) from the encoder. It also often uses its *own previously generated outputs* to decide what to generate next, building the output sequence step by step.\n*   **Output:** A new sequence or piece of data that is a transformation of the original input (e.g., the sentence translated into English, a caption describing the image, a summarized paragraph).\n*   **Analogy:** Now that your brain (the encoder) has formed the internal \"thought\" from the French sentence, your brain (now acting as the decoder) takes that thought and constructs an English sentence to express it.\n\n---\n\n### **The Key Difference in Simple Terms:**\n\n*   **Encoder:** **Understands** the input and turns it into a compact, meaningful internal representation.\n    *   *Role:* Compression, Feature Extraction, Understanding.\n*   **Decoder:** **Generates** an output sequence based on that internal representation.\n    *   *Role:* Generation, Reconstruction, Translation.\n\n**Think of it like this:**\n\n1.  **Encoder:** You read a complicated instruction manual and distill its main points into a single mental note.\n2.  **Decoder:** You take that mental note and write a simple, easy-to-follow instruction for someone else.\n\nThey are often used together in **\"sequence-to-sequence\" (Seq2Seq)** models for tasks like machine translation, text summarization, image captioning, and speech recognition. The encoder processes the input sequence, and the decoder generates the output sequence based on the encoder's understanding."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK-kjmMoi5rO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "fe927d79-6f20-4afc-ed1a-f0ecfce2ad74"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Generative AI are models trained on large datasets to create new, original content (text, images, music, code) that resembles the training material.\n\n- It uses big neural networks (like transformers or diffusion models) to predict or sample data (e.g., next word, denoise an image) guided by prompts, enabling creative outputs along with challenges like bias, hallucinations, and copyright concerns."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = get_completion(prompt='Explain Generative AI in 2 bullet points')   # using the gpt model by default\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(prompt='Explain AI Agents in 5 bullet points', model=\"gemini\") #\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "id": "l6se9pA9Lf_F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "c7096fbf-bd1b-4cfa-cf8d-f50207584320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here are 5 bullet points explaining AI Agents:\n\n*   **Autonomous & Goal-Oriented:** AI Agents are autonomous software entities designed to achieve specific goals or perform tasks without continuous human intervention.\n*   **Perceive, Process, Act:** They operate based on a continuous loop: perceiving their environment, processing that information, making decisions based on their goals, and then taking actions within that environment.\n*   **Reasoning & Planning:** Modern AI agents often leverage Large Language Models (LLMs) for advanced reasoning, planning complex multi-step tasks, and understanding natural language instructions.\n*   **Tool Utilization:** They can integrate with and utilize external tools, APIs, or databases to extend their capabilities beyond their core AI, allowing them to interact with the real world (e.g., browse the web, send emails, run code).\n*   **Adaptive & Diverse Applications:** They are designed to adapt to dynamic environments and are used in various fields, from automating workflows and managing cloud infrastructure to controlling robots and powering advanced chatbots."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmnwGrskn_oz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a0TfeZC7FQTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-O3ubnufFQQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Anthropic Models"
      ],
      "metadata": {
        "id": "2WBVhBKdFRk-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6NDLIaTFgzgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c9aa021-7bbd-426d-c948-bc0af03ce5c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/405.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.9/405.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install anthropic --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VGx9M87ggzgF"
      },
      "outputs": [],
      "source": [
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get('ANTHROPIC_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "aLHqAhm1gzgG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86bbe45c-e7a7-4024-d31a-bbf0103b9677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Agentic AI\n",
            "\n",
            "• **Autonomous decision-making**: AI systems that can independently set goals, make decisions, and take actions without constant human intervention or step-by-step instructions.\n",
            "\n",
            "• **Proactive problem-solving**: Unlike traditional AI that simply responds to prompts, agentic AI can identify problems, plan multi-step solutions, and execute tasks to achieve specified objectives.\n",
            "\n",
            "• **Tool use and interaction**: These systems can interact with external tools, APIs, databases, and software applications to gather information and accomplish real-world tasks beyond text generation.\n",
            "\n",
            "• **Adaptive learning and reasoning**: Agentic AI can assess outcomes, learn from feedback, adjust strategies when approaches fail, and iteratively improve its methods to reach goals.\n",
            "\n",
            "• **Multi-step task execution**: Capable of breaking down complex objectives into subtasks, maintaining context across long sequences of actions, and orchestrating workflows that may take extended time to complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from anthropic import Anthropic\n",
        "\n",
        "client = Anthropic()\n",
        "message = client.messages.create(\n",
        "    max_tokens=1024,\n",
        "    messages=[{\n",
        "        \"content\": \"Explain Agentic AI in 5 bullet points.\",\n",
        "        \"role\": \"user\",\n",
        "    }],\n",
        "    model=\"claude-sonnet-4-5\",\n",
        ")\n",
        "# print(message.id)\n",
        "print(message.content[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8MD1AgaUDZ_c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "92f391cc-0e26-4930-dee7-4d3dd561dbeb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Agentic AI\n\n• **Autonomous decision-making**: AI systems that can independently set goals, make decisions, and take actions without constant human intervention or step-by-step instructions.\n\n• **Proactive problem-solving**: Unlike traditional AI that simply responds to prompts, agentic AI can identify problems, plan multi-step solutions, and execute tasks to achieve specified objectives.\n\n• **Tool use and interaction**: These systems can interact with external tools, APIs, databases, and software applications to gather information and accomplish real-world tasks beyond text generation.\n\n• **Adaptive learning and reasoning**: Agentic AI can assess outcomes, learn from feedback, adjust strategies when approaches fail, and iteratively improve its methods to reach goals.\n\n• **Multi-step task execution**: Capable of breaking down complex objectives into subtasks, maintaining context across long sequences of actions, and orchestrating workflows that may take extended time to complete."
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(message.content[0].text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  HW update the above \"get_completions\" function with this Anthropic code"
      ],
      "metadata": {
        "id": "5O_vSw6sIEKF"
      },
      "execution_count": 28,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}